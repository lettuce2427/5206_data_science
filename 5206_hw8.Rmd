---
title: "5206_hw8"
author: "Peiru Wu(pw2427)"
date: "November 14, 2016"

output: pdf_document
---
1.
```{r}
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p-s))
y <- x %*% b + rt(n, df=2)
cov(x,y)





```

```{r,tidy=T}
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p),n,p)
b <- c(-0.7,0.7,1,rep(0,p-s))
y <- x%*%b +rt(n,df=2)
cov(x,y)
# I am afraid I cannot pick out these 3 relevant variables based on correlations alone, because there is no distinct difference.
```
2.
```{r,tidy=T}
x1 <- seq(-10,10,0.01)
plot(x1,dnorm(x1),type="l",ylab="Density",xlab="x",col="blue")
points(x1,dt(x1,3),type="l",col="red")
# The blue line represents normal distribution and the red line represents T-distribution random varibles.
```
3.
```{r,tidy=T}
psi <- function(r, c=1) {
  return(ifelse(r^2 > c^2, 2*c*abs(r) - c^2, r^2))
}
huber.loss <- function(beta){
  return ( sum(psi(y - x %*% beta)) )
}
```
4.
```{r,tidy=TRUE}
install.packages("numDeriv")
library(numDeriv)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
  n    <- length(x0)
  xmat <- matrix(0, nrow = n, ncol = max.iter)
  xmat[,1] <- x0
  
  for (k in 2:max.iter) {
    # Calculate the gradient
    grad.cur <- grad(f, xmat[ ,k-1], ...) 
    
    # Should we stop?
    if (all(abs(grad.cur) < stopping.deriv)) {
      k <- k-1; break
    }
    
    # Move in the opposite direction of the grad
    xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
  } 
  xmat <- xmat[ ,1:k] # Trim
  return(list(x = xmat[,k], xmat = xmat, k = k))
}
grad.descent(huber.loss,x0=rep(0,p))  




```

```{r,tidy=T}
#install.packages("numDeriv")
library(numDeriv)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
  
  n    <- length(x0)
  xmat <- matrix(0, nrow = n, ncol = max.iter)
  xmat[,1] <- x0
  
  for (k in 2:max.iter) {
    # Calculate the gradient
    grad.cur <- grad(f, xmat[ ,k-1], ...) 
    if (all(abs(grad.cur) < stopping.deriv)) {
      k <- k-1; break
    }
    
    # Move in the opposite direction of the grad
    xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
  }
  
  xmat <- xmat[ ,1:k] # Trim
  return(list(x = xmat[,k], xmat = xmat, k = k))
}

gd <- grad.descent(huber.loss, x0 = rep(0, p), max.iter = 200, step.size = 0.001, stopping.deriv = 0.1)
gd$k 
# Number of iterations to converge
gd$x 
# The final coefficient estimates
```
5.
```{r, tidy = T}
obj <- apply(psi(matrix(rep(y, gd$k), nrow = 100) - x %*% gd$xmat), 2, sum)
plot(1:gd$k, obj, type = 'l', xlab = "Iteration", ylab = "Objective Value Function", main = "Gradient Descent with Step Size 0.001")
# The object value decreases rapidly at the start, but the descending speed rapidly slows down. From the plot, there is decrease with lower and lower speed and nearly 0 after 60 iterations.
```
6.
```{r, tidy = T}
gd2 <- grad.descent(huber.loss, x0 = rep(0, p), max.iter = 200, step.size = 0.1, stopping.deriv = 0.1)
obj2 <- apply(psi(matrix(rep(y, gd2$k), nrow = 100) - x %*% gd2$xmat), 2, sum)
plot((gd2$k-49):gd2$k, obj2[151:gd2$k], type = 'l', xlab = "Iteration", ylab = "Objective Value Function", main = "Gradient Descent with Step Size 0.1 (last 50 iteration)")
# The criterion value fluctuates instead of monotonically decreases at each step, and till the end it hasn't converged. The change of coefficients are drastic and periodical, but never converge to some stable value as the final guess of the optimized results, which can also be confirmed at gd2$xmat.
```
7.
```{r, tidy = T}
sparse.grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, threshold = 0.05, ...) {
  
  n    <- length(x0)
  xmat <- matrix(0, nrow = n, ncol = max.iter)
  xmat[,1] <- x0
  
  for (k in 2:max.iter) {
    # Calculate the gradient
    grad.cur <- grad(f, xmat[ ,k-1], ...) 
    if (all(abs(grad.cur) < stopping.deriv)) {
      k <- k-1; break
    }
    
    # Move in the opposite direction of the grad
    new_coef = xmat[ ,k-1] - step.size * grad.cur
    
    # Threshold small values in these coefficients to zero
    xmat[ ,k] <- new_coef * (abs(new_coef) > threshold)
  }
  
  xmat <- xmat[ ,1:k] # Trim
  return(list(x = xmat[,k], xmat = xmat, k = k))
}

gd.sparse = sparse.grad.descent(huber.loss, x0 = rep(0, p), max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, threshold = 0.05)
b  
# b is real coefficient
gd.sparse$x 
# final coefficients estimated
b-gd.sparse$x
# The last 7 value of the optimized estimate coefficients becomes zero as expected
```

8.
```{r, tidy = T}
m1 <- lm(y ~ x)
MSE <- function(coef) { 
  return(sum((coef-b)^2))
}
m1_coef <-vector()
for(i in 1:10){
  m1_coef[i]<-m1$coefficients[i+1]
}
unname(m1_coef)
MSE(coef(m1)[2:11]) + coef(m1)[1]^2 #MLE of m1
MSE(gd$x)
MSE(gd.sparse$x)

# The result from lm() is much less accurate compared to coefficients estimated from question 4 and question 7.
# The one from question 7 has the smallest MSE value.
```

9.
```{r, tidy = T}
set.seed(10)
y <- x %*% b + rt(n, df=2)
gd.new <- grad.descent(huber.loss, x0 = rep(0, p), max.iter = 200, step.size = 0.001, stopping.deriv = 0.1)
gd.sparse.new <- sparse.grad.descent(huber.loss, x0 = rep(0, p), max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, threshold = 0.05)

esti_coef <- round(rbind(gd$x, gd.new$x, gd.sparse$x, gd.sparse.new$x), 2)
rownames(esti_coef) <- c("gd", "gd.new", "gd.sparse", "gd.sparse.new")
esti_coef
# The new coefficients from gradient descent change greatly from the former one. As for the sparsified gradient descent, the second and third coefficients remain similar while the first coefficient becomes zero (which is not what we want,  showing that the sparse method is not very robust).
MSE(gd$x)
MSE(gd.sparse$x)
MSE(gd.new$x)
MSE(gd.sparse.new$x)

# In terms of MSE measured against the b, The gd.new is still smaller than gd.sparse.new model. It deduces that the estimates from the sparse method has higher variability, but less robust.
```

10.
```{r, tidy = T}
mse_mat <- matrix(nrow = 2, ncol = 10)
rownames(mse_mat) <- c("gd", "gd.sparse")
for (i in 1:10){
  y <- x %*% b + rt(n, df=2)
  gd.new <- grad.descent(huber.loss, x0 = rep(0, p), max.iter = 200, step.size = 0.001, stopping.deriv = 0.1)
  gd.sparse.new <- sparse.grad.descent(huber.loss, x0 = rep(0, p), max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, threshold = 0.05)
  mse_mat[,i] <- c(MSE(gd.new$x), MSE(gd.sparse.new$x))
}

apply(mse_mat, 1, mean) # average MSE

apply(mse_mat, 1, min) # min MSE

# Thus the initial gradient descent has smaller average MSE, and the initial gradient descent also has smaller min MSE. This is in line with my interpretation of the variability associated with the sparse gradient descent method.
```
```